{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b54b676",
   "metadata": {},
   "source": [
    "# Handwritten Character Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34c48446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import relu,linear\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049bf93",
   "metadata": {},
   "source": [
    "## The Data\n",
    "The data was taken from [this](https://www.kaggle.com/datasets/sachinpatel21/az-handwritten-alphabets-in-csv-format) Kaggle dataset, which provides a csv format of the original data from the [NIST](https://www.nist.gov/srd/nist-special-database-19) and MNIST datasets. \n",
    "<br>\n",
    "The data represents 372451 images drawn on a 28x28 grid. The 1st column contains the classification label (0->25 representing A->Z), and the rest of the 784 columns contain the input features (numbers from 0->255 represnting the color degree of the pixel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec53107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record: 0\n",
      "record: 10000\n",
      "record: 20000\n",
      "record: 30000\n",
      "record: 40000\n",
      "record: 50000\n",
      "record: 60000\n",
      "record: 70000\n",
      "record: 80000\n",
      "record: 90000\n",
      "record: 100000\n",
      "record: 110000\n",
      "record: 120000\n",
      "record: 130000\n",
      "record: 140000\n",
      "record: 150000\n",
      "record: 160000\n",
      "record: 170000\n",
      "record: 180000\n",
      "record: 190000\n",
      "record: 200000\n",
      "record: 210000\n",
      "record: 220000\n",
      "record: 230000\n",
      "record: 240000\n",
      "record: 250000\n",
      "record: 260000\n",
      "record: 270000\n",
      "record: 280000\n",
      "record: 290000\n",
      "record: 300000\n",
      "record: 310000\n",
      "record: 320000\n",
      "record: 330000\n",
      "record: 340000\n",
      "record: 350000\n",
      "record: 360000\n",
      "record: 370000\n",
      "(372451,)\n",
      "(372451, 784)\n",
      "25.0\n"
     ]
    }
   ],
   "source": [
    "# Open the CSV file\n",
    "with open('A_Z handwritten Data.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = []\n",
    "    for i, row in enumerate(reader):\n",
    "        if i % 50000 == 0:\n",
    "            print(f'iteration: {i}')\n",
    "        data.append(row)\n",
    "    data = np.array(data)\n",
    "\n",
    "# Get the first column and store it in a numpy array\n",
    "y = np.array(data[:, 0], dtype=float)\n",
    "\n",
    "# Get the rest of the columns and store it in a numpy matrix\n",
    "X = np.matrix(data[:, 1:], dtype=float)\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414a4cb",
   "metadata": {},
   "source": [
    "## Splitting our Data\n",
    "We split our dataset into training and test sets so we can later compare the missclassifications to check for the presence of bias and variance issues. \n",
    "<br>\n",
    "Training Set: 75%\n",
    "<br>\n",
    "Test Set: 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb18a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7646e",
   "metadata": {},
   "source": [
    "## The Model\n",
    "### Layers\n",
    "I tested different models with different layer strctures, 100-100-100, 500-200-100, 784-784-784 etc. Increasing the number of nodes per layer didnt seem to help reduce the cost/loss much beyond 100 layers. The model I ended up staying with was 100-100-100-100 (4 layers of 100 nodes and then a final layer of 26 nodes).\n",
    "### Regulaizer\n",
    "Using a regulaizer was causing the model to underfit the data, and the absence of one wasn't causing an overfitting issue, so ended up not using one.\n",
    "### Loss & Optimizer\n",
    "Used the Sparse Categorical Crossentropy loss function because this is a multi-class classification problem along with the setting to let tensorflow know to use the sigmoid function since the layer is set to a linear activation for numerical accuracy.\n",
    "<br>\n",
    "Used the Adam optimizer with a small learning rate of 0.0001 since larger values seemed to cause gradient descent to not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c00f235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(26, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17534857",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=Adam(learning_rate=0.0001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fee181c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8730/8730 [==============================] - 39s 4ms/step - loss: 1.4177\n",
      "Epoch 2/30\n",
      "8730/8730 [==============================] - 36s 4ms/step - loss: 0.3008\n",
      "Epoch 3/30\n",
      "8730/8730 [==============================] - 36s 4ms/step - loss: 0.2031\n",
      "Epoch 4/30\n",
      "8730/8730 [==============================] - 37s 4ms/step - loss: 0.1575\n",
      "Epoch 5/30\n",
      "8730/8730 [==============================] - 37s 4ms/step - loss: 0.1307\n",
      "Epoch 6/30\n",
      "8730/8730 [==============================] - 37s 4ms/step - loss: 0.1108\n",
      "Epoch 7/30\n",
      "8730/8730 [==============================] - 37s 4ms/step - loss: 0.0961\n",
      "Epoch 8/30\n",
      "8730/8730 [==============================] - 37s 4ms/step - loss: 0.0843\n",
      "Epoch 9/30\n",
      "8730/8730 [==============================] - 39s 4ms/step - loss: 0.0753\n",
      "Epoch 10/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0673\n",
      "Epoch 11/30\n",
      "8730/8730 [==============================] - 38s 4ms/step - loss: 0.0603\n",
      "Epoch 12/30\n",
      "8730/8730 [==============================] - 38s 4ms/step - loss: 0.0548\n",
      "Epoch 13/30\n",
      "8730/8730 [==============================] - 38s 4ms/step - loss: 0.0508\n",
      "Epoch 14/30\n",
      "8730/8730 [==============================] - 39s 5ms/step - loss: 0.0455\n",
      "Epoch 15/30\n",
      "8730/8730 [==============================] - 38s 4ms/step - loss: 0.0423\n",
      "Epoch 16/30\n",
      "8730/8730 [==============================] - 39s 4ms/step - loss: 0.0392\n",
      "Epoch 17/30\n",
      "8730/8730 [==============================] - 39s 4ms/step - loss: 0.0359\n",
      "Epoch 18/30\n",
      "8730/8730 [==============================] - 39s 4ms/step - loss: 0.0348\n",
      "Epoch 19/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0323\n",
      "Epoch 20/30\n",
      "8730/8730 [==============================] - 42s 5ms/step - loss: 0.0309\n",
      "Epoch 21/30\n",
      "8730/8730 [==============================] - 41s 5ms/step - loss: 0.0282\n",
      "Epoch 22/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0281\n",
      "Epoch 23/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0263\n",
      "Epoch 24/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0259\n",
      "Epoch 25/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0240\n",
      "Epoch 26/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0231\n",
      "Epoch 27/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0233\n",
      "Epoch 28/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0221\n",
      "Epoch 29/30\n",
      "8730/8730 [==============================] - 41s 5ms/step - loss: 0.0216\n",
      "Epoch 30/30\n",
      "8730/8730 [==============================] - 43s 5ms/step - loss: 0.0207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f320f070>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd8628",
   "metadata": {},
   "source": [
    "## Testing our Model\n",
    "To test our model, we can simply loop over our dataset and see how many values it correctly classifies (or miss-classifies for that matter). We do the same for our test set so we can compare these values and see if our model generalizes well to data it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ba8c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8730/8730 [==============================] - 20s 2ms/step\n",
      "accuracy with trainingt set: 0.9906743801416206\n",
      "2910/2910 [==============================] - 6s 2ms/step\n",
      "accuracy with test set: 0.9783918464661218\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_train)\n",
    "correct = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == np.argmax(tf.nn.softmax(yhat[i])):\n",
    "        correct += 1\n",
    "print('accuracy with trainingt set: ' + str(correct/len(y_train)))\n",
    "\n",
    "yhat = model.predict(X_test)\n",
    "correct = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == np.argmax(tf.nn.softmax(yhat[i])):\n",
    "        correct += 1\n",
    "print('accuracy with test set: ' + str(correct/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ade4a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "On our training set, the model predicted ~ 99.07% of the labels correctly. On our test set, our model predicted ~ 97.84% of the labels correctly. The model does seem to generalize out to unseen data pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60b17b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446eab75",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "We can run some code on our test set predictions to check if there is a pattern on letters that our model has trouble predicting. We can use this information to further tune our model in the future to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "76013336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 71\n",
      "1: 75\n",
      "2: 71\n",
      "3: 65\n",
      "4: 88\n",
      "5: 6\n",
      "6: 46\n",
      "7: 153\n",
      "8: 11\n",
      "9: 65\n",
      "10: 58\n",
      "11: 55\n",
      "12: 53\n",
      "13: 42\n",
      "14: 333\n",
      "15: 39\n",
      "16: 60\n",
      "17: 105\n",
      "18: 80\n",
      "19: 60\n",
      "20: 119\n",
      "21: 14\n",
      "22: 192\n",
      "23: 37\n",
      "24: 90\n",
      "25: 24\n"
     ]
    }
   ],
   "source": [
    "incorrect_count = [0] * 26\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] != np.argmax(tf.nn.softmax(yhat[i])):\n",
    "        incorrect_count[int(y_test[i])] += 1\n",
    "for i in range(26):\n",
    "    print(f'{i}: {incorrect_count[i]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea45117",
   "metadata": {},
   "source": [
    "From the looks of it, the missclassifications are split pretty evenly among all the letters except for a few anomalies. \n",
    "<br>\n",
    "It has the most issues with predicting the letters 'O', 'W', and 'H' in with 333, 192 & 153 missclassificataions respectively. Note the average for each letter seems to be around ~ 60.\n",
    "<br>\n",
    "On the other hand, it almost always correctly classified 'F', 'I', and 'V', with only 6, 11, and 14 missclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148409c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
