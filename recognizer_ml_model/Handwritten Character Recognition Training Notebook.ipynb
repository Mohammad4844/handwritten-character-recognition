{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b54b676",
   "metadata": {},
   "source": [
    "# Handwritten Character Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c48446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 23:26:41.807875: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import relu,linear\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049bf93",
   "metadata": {},
   "source": [
    "## The Data\n",
    "The data was taken from [this](https://www.kaggle.com/datasets/sachinpatel21/az-handwritten-alphabets-in-csv-format) Kaggle dataset, which provides a csv format of the original data from the [NIST](https://www.nist.gov/srd/nist-special-database-19) and MNIST datasets. \n",
    "<br>\n",
    "The data represents 372451 images drawn on a 28x28 grid. The 1st column contains the classification label (0->25 representing A->Z), and the rest of the 784 columns contain the input features (numbers from 0->255 represnting the color degree of the pixel).\n",
    "<br>\n",
    "*The next cell takes some time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec53107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "iteration: 50000\n",
      "iteration: 100000\n",
      "iteration: 150000\n",
      "iteration: 200000\n",
      "iteration: 250000\n",
      "iteration: 300000\n",
      "iteration: 350000\n",
      "(372451,)\n",
      "(372451, 784)\n"
     ]
    }
   ],
   "source": [
    "# Open the CSV file\n",
    "with open('A_Z handwritten Data.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = []\n",
    "    for i, row in enumerate(reader):\n",
    "        if i % 50000 == 0:\n",
    "            print(f'iteration: {i}')\n",
    "        data.append(row)\n",
    "    data = np.array(data)\n",
    "\n",
    "# Get the first column and store it in a numpy array\n",
    "y = np.array(data[:, 0], dtype=float)\n",
    "\n",
    "# Get the rest of the columns and store it in a numpy matrix\n",
    "X = np.matrix(data[:, 1:], dtype=float)\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414a4cb",
   "metadata": {},
   "source": [
    "## Splitting our Data\n",
    "We split our dataset into training and test sets so we can later compare the missclassifications to check for the presence of bias and variance issues. \n",
    "<br>\n",
    "Training Set: 75%\n",
    "<br>\n",
    "Test Set: 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb18a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7646e",
   "metadata": {},
   "source": [
    "## The Model\n",
    "### Layers\n",
    "I tested different models with different layer strctures, 100-100-100, 500-200-100, 784-784-784 etc. Increasing the number of nodes per layer didnt seem to help reduce the cost/loss much beyond 100 layers. The model I ended up staying with was 100-100-100-100 (4 layers of 100 nodes and then a final layer of 26 nodes).\n",
    "### Regulaizer\n",
    "Using a regulaizer was causing the model to underfit the data, and the absence of one wasn't causing an overfitting issue, so ended up not using one.\n",
    "### Loss & Optimizer\n",
    "Used the Sparse Categorical Crossentropy loss function because this is a multi-class classification problem along with the setting to let tensorflow know to use the sigmoid function since the layer is set to a linear activation for numerical accuracy.\n",
    "<br>\n",
    "Used the Adam optimizer with a small learning rate of 0.0001 since larger values seemed to cause gradient descent to not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00f235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 23:33:50.551222: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(26, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17534857",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=Adam(learning_rate=0.0001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee181c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8730/8730 [==============================] - 34s 4ms/step - loss: 1.3185\n",
      "Epoch 2/30\n",
      "8730/8730 [==============================] - 36s 4ms/step - loss: 0.3043\n",
      "Epoch 3/30\n",
      "8730/8730 [==============================] - 54s 6ms/step - loss: 0.2026\n",
      "Epoch 4/30\n",
      "8730/8730 [==============================] - 65s 7ms/step - loss: 0.1554\n",
      "Epoch 5/30\n",
      "8730/8730 [==============================] - 32s 4ms/step - loss: 0.1264\n",
      "Epoch 6/30\n",
      "8730/8730 [==============================] - 31s 4ms/step - loss: 0.1061\n",
      "Epoch 7/30\n",
      "8730/8730 [==============================] - 29s 3ms/step - loss: 0.0914\n",
      "Epoch 8/30\n",
      "8730/8730 [==============================] - 30s 3ms/step - loss: 0.0798\n",
      "Epoch 9/30\n",
      "8730/8730 [==============================] - 31s 4ms/step - loss: 0.0708\n",
      "Epoch 10/30\n",
      "8730/8730 [==============================] - 33s 4ms/step - loss: 0.0626\n",
      "Epoch 11/30\n",
      "8730/8730 [==============================] - 31s 4ms/step - loss: 0.0570\n",
      "Epoch 12/30\n",
      "8730/8730 [==============================] - 29s 3ms/step - loss: 0.0516\n",
      "Epoch 13/30\n",
      "8730/8730 [==============================] - 31s 4ms/step - loss: 0.0472\n",
      "Epoch 14/30\n",
      "8730/8730 [==============================] - 31s 4ms/step - loss: 0.0435\n",
      "Epoch 15/30\n",
      "8730/8730 [==============================] - 30s 3ms/step - loss: 0.0398\n",
      "Epoch 16/30\n",
      "8730/8730 [==============================] - 32s 4ms/step - loss: 0.0372\n",
      "Epoch 17/30\n",
      "8730/8730 [==============================] - 32s 4ms/step - loss: 0.0342\n",
      "Epoch 18/30\n",
      "8730/8730 [==============================] - 33s 4ms/step - loss: 0.0326\n",
      "Epoch 19/30\n",
      "8730/8730 [==============================] - 48s 5ms/step - loss: 0.0309\n",
      "Epoch 20/30\n",
      "8730/8730 [==============================] - 41s 5ms/step - loss: 0.0292\n",
      "Epoch 21/30\n",
      "8730/8730 [==============================] - 38s 4ms/step - loss: 0.0275\n",
      "Epoch 22/30\n",
      "8730/8730 [==============================] - 39s 4ms/step - loss: 0.0265\n",
      "Epoch 23/30\n",
      "8730/8730 [==============================] - 41s 5ms/step - loss: 0.0247\n",
      "Epoch 24/30\n",
      "8730/8730 [==============================] - 39s 4ms/step - loss: 0.0247\n",
      "Epoch 25/30\n",
      "8730/8730 [==============================] - 41s 5ms/step - loss: 0.0232\n",
      "Epoch 26/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0223\n",
      "Epoch 27/30\n",
      "8730/8730 [==============================] - 42s 5ms/step - loss: 0.0217\n",
      "Epoch 28/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0201\n",
      "Epoch 29/30\n",
      "8730/8730 [==============================] - 40s 5ms/step - loss: 0.0205\n",
      "Epoch 30/30\n",
      "8730/8730 [==============================] - 39s 5ms/step - loss: 0.0196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2536ef8e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd8628",
   "metadata": {},
   "source": [
    "## Testing our Model\n",
    "To test our model, we can simply loop over our dataset and see how many values it correctly classifies (or miss-classifies for that matter). We do the same for our test set so we can compare these values and see if our model generalizes well to data it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba8c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8730/8730 [==============================] - 24s 3ms/step\n",
      "accuracy with trainingt set: 0.9945585634607537\n",
      "2910/2910 [==============================] - 5s 2ms/step\n",
      "accuracy with test set: 0.9821507200927905\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "yhat = model.predict(X_train)\n",
    "correct = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == np.argmax(tf.nn.softmax(yhat[i])):\n",
    "        correct += 1\n",
    "print('accuracy with trainingt set: ' + str(correct/len(y_train)))\n",
    "\n",
    "# test set\n",
    "yhat = model.predict(X_test)\n",
    "correct = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == np.argmax(tf.nn.softmax(yhat[i])):\n",
    "        correct += 1\n",
    "print('accuracy with test set: ' + str(correct/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ade4a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "On our training set, the model predicted ~ 99.4% of the labels correctly. On our test set, our model predicted ~ 98.2% of the labels correctly. The model does seem to generalize out to unseen data pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60b17b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446eab75",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "We can run some code on our test set predictions to check if there is a pattern on letters that our model has trouble predicting. We can use this information to further tune our model in the future to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76013336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 56\n",
      "1: 50\n",
      "2: 52\n",
      "3: 119\n",
      "4: 40\n",
      "5: 11\n",
      "6: 64\n",
      "7: 80\n",
      "8: 14\n",
      "9: 97\n",
      "10: 44\n",
      "11: 35\n",
      "12: 77\n",
      "13: 97\n",
      "14: 136\n",
      "15: 64\n",
      "16: 104\n",
      "17: 63\n",
      "18: 61\n",
      "19: 26\n",
      "20: 115\n",
      "21: 40\n",
      "22: 52\n",
      "23: 40\n",
      "24: 96\n",
      "25: 29\n"
     ]
    }
   ],
   "source": [
    "incorrect_count = [0] * 26\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] != np.argmax(tf.nn.softmax(yhat[i])):\n",
    "        incorrect_count[int(y_test[i])] += 1\n",
    "for i in range(26):\n",
    "    print(f'{i}: {incorrect_count[i]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea45117",
   "metadata": {},
   "source": [
    "From the looks of it, the missclassifications are split pretty evenly among all the letters except for a few anomalies. \n",
    "<br>\n",
    "It has the most issues with predicting the letters 'O', 'D', and 'U' in with 136, 119 & 115 missclassificataions respectively. Note the average for each letter seems to be around ~ 50.\n",
    "<br>\n",
    "On the other hand, it almost always correctly classified 'F' and 'I' with only 11 and 14 missclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148409c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
